{
  "data": [
    {
      "id": "qwen/qwen3-32b",
      "canonical_slug": "qwen/qwen3-32b-04-28",
      "hugging_face_id": "Qwen/Qwen3-32B",
      "name": "Qwen: Qwen3 32B",
      "created": 1745875945,
      "description": "Qwen3-32B is a dense 32.8B parameter causal language model from the Qwen3 series, optimized for both complex reasoning and efficient dialogue. It supports seamless switching between a \"thinking\" mode for tasks like math, coding, and logical inference, and a \"non-thinking\" mode for faster, general-purpose conversation. The model demonstrates strong performance in instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling. ",
      "context_length": 40960,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Qwen3",
        "instruct_type": "qwen3"
      },
      "pricing": {
        "prompt": "0.00000008",
        "completion": "0.00000024",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 40960,
        "max_completion_tokens": 40960,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {}
    },
    {
      "id": "moonshotai/kimi-k2-thinking",
      "canonical_slug": "moonshotai/kimi-k2-thinking-20251106",
      "hugging_face_id": "moonshotai/Kimi-K2-Thinking",
      "name": "MoonshotAI: Kimi K2 Thinking",
      "created": 1762440622,
      "description": "Kimi K2 Thinking is Moonshot AI’s most advanced open reasoning model to date, extending the K2 series into agentic, long-horizon reasoning. Built on the trillion-parameter Mixture-of-Experts (MoE) architecture introduced in Kimi K2, it activates 32 billion parameters per forward pass and supports 256 k-token context windows. The model is optimized for persistent step-by-step thought, dynamic tool invocation, and complex reasoning workflows that span hundreds of turns. It interleaves step-by-step reasoning with tool use, enabling autonomous research, coding, and writing that can persist for hundreds of sequential actions without drift.\n\nIt sets new open-source benchmarks on HLE, BrowseComp, SWE-Multilingual, and LiveCodeBench, while maintaining stable multi-agent behavior through 200–300 tool calls. Built on a large-scale MoE architecture with MuonClip optimization, it combines strong reasoning depth with high inference efficiency for demanding agentic and analytical tasks.",
      "context_length": 262144,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000004",
        "completion": "0.00000175",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": 65535,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      }
    },
    {
      "id": "openai/gpt-oss-120b",
      "canonical_slug": "openai/gpt-oss-120b",
      "hugging_face_id": "openai/gpt-oss-120b",
      "name": "OpenAI: gpt-oss-120b",
      "created": 1754414231,
      "description": "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000002",
        "completion": "0.0000001",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "reasoning_effort",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      }
    },
    {
      "id": "moonshotai/kimi-k2-0905",
      "canonical_slug": "moonshotai/kimi-k2-0905",
      "hugging_face_id": "moonshotai/Kimi-K2-Instruct-0905",
      "name": "MoonshotAI: Kimi K2 0905",
      "created": 1757021147,
      "description": "Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k.\n\nThis update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. Kimi K2 is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. It excels across coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) benchmarks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.",
      "context_length": 262144,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000039",
        "completion": "0.0000019",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": 262144,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {}
    },
    {
      "id": "google/gemini-3-flash-preview",
      "canonical_slug": "google/gemini-3-flash-preview-20251217",
      "hugging_face_id": "",
      "name": "Google: Gemini 3 Flash Preview",
      "created": 1765987078,
      "description": "Gemini 3 Flash Preview is a high speed, high value thinking model designed for agentic workflows, multi turn chat, and coding assistance. It delivers near Pro level reasoning and tool use performance with substantially lower latency than larger Gemini variants, making it well suited for interactive development, long running agent loops, and collaborative coding tasks. Compared to Gemini 2.5 Flash, it provides broad quality improvements across reasoning, multimodal understanding, and reliability.\n\nThe model supports a 1M token context window and multimodal inputs including text, images, audio, video, and PDFs, with text output. It includes configurable reasoning via thinking levels (minimal, low, medium, high), structured output, tool use, and automatic context caching. Gemini 3 Flash Preview is optimized for users who want strong reasoning and agentic behavior without the cost or latency of full scale frontier models.",
      "context_length": 1048576,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": ["text", "image", "file", "audio", "video"],
        "output_modalities": ["text"],
        "tokenizer": "Gemini",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000005",
        "completion": "0.000003",
        "request": "0",
        "image": "0",
        "audio": "0.000001",
        "web_search": "0",
        "internal_reasoning": "0",
        "input_cache_read": "0.00000005"
      },
      "top_provider": {
        "context_length": 1048576,
        "max_completion_tokens": 65535,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      }
    },
    {
      "id": "qwen/qwen3-next-80b-a3b-instruct",
      "canonical_slug": "qwen/qwen3-next-80b-a3b-instruct-2509",
      "hugging_face_id": "Qwen/Qwen3-Next-80B-A3B-Instruct",
      "name": "Qwen: Qwen3 Next 80B A3B Instruct",
      "created": 1757612213,
      "description": "Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model in the Qwen3-Next series optimized for fast, stable responses without “thinking” traces. It targets complex tasks across reasoning, code generation, knowledge QA, and multilingual use, while remaining robust on alignment and formatting. Compared with prior Qwen3 instruct variants, it focuses on higher throughput and stability on ultra-long inputs and multi-turn dialogues, making it well-suited for RAG, tool use, and agentic workflows that require consistent final answers rather than visible chain-of-thought.\n\nThe model employs scaling-efficient training and decoding to improve parameter efficiency and inference speed, and has been validated on a broad set of public benchmarks where it reaches or approaches larger Qwen3 systems in several categories while outperforming earlier mid-sized baselines. It is best used as a general assistant, code helper, and long-context task solver in production settings where deterministic, instruction-following outputs are preferred.",
      "context_length": 262144,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Qwen3",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000006",
        "completion": "0.0000006",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {}
    },
    {
      "id": "qwen/qwen3-vl-235b-a22b-instruct",
      "canonical_slug": "qwen/qwen3-vl-235b-a22b-instruct",
      "hugging_face_id": "Qwen/Qwen3-VL-235B-A22B-Instruct",
      "name": "Qwen: Qwen3 VL 235B A22B Instruct",
      "created": 1758668687,
      "description": "Qwen3-VL-235B-A22B Instruct is an open-weight multimodal model that unifies strong text generation with visual understanding across images and video. The Instruct model targets general vision-language use (VQA, document parsing, chart/table extraction, multilingual OCR). The series emphasizes robust perception (recognition of diverse real-world and synthetic categories), spatial understanding (2D/3D grounding), and long-form visual comprehension, with competitive results on public multimodal benchmarks for both perception and reasoning.\n\nBeyond analysis, Qwen3-VL supports agentic interaction and tool use: it can follow complex instructions over multi-image, multi-turn dialogues; align text to video timelines for precise temporal queries; and operate GUI elements for automation tasks. The models also enable visual coding workflows—turning sketches or mockups into code and assisting with UI debugging—while maintaining strong text-only performance comparable to the flagship Qwen3 language models. This makes Qwen3-VL suitable for production scenarios spanning document AI, multilingual OCR, software/UI assistance, spatial/embodied tasks, and research on vision-language agents.",
      "context_length": 262144,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": ["text", "image"],
        "output_modalities": ["text"],
        "tokenizer": "Qwen3",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000002",
        "completion": "0.0000012",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 0.7,
        "top_p": 0.8,
        "frequency_penalty": null
      }
    },
    {
      "id": "z-ai/glm-4.7",
      "canonical_slug": "z-ai/glm-4.7-20251222",
      "hugging_face_id": "zai-org/GLM-4.7",
      "name": "Z.AI: GLM 4.7",
      "created": 1766378014,
      "description": "GLM-4.7 is Z.AI’s latest flagship model, featuring upgrades in two key areas: enhanced programming capabilities and more stable multi-step reasoning/execution. It demonstrates significant improvements in executing complex agent tasks while delivering more natural conversational experiences and superior front-end aesthetics.",
      "context_length": 202752,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000004",
        "completion": "0.0000015",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 202752,
        "max_completion_tokens": 65535,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 1,
        "top_p": 0.95,
        "frequency_penalty": null
      }
    },
    {
      "id": "deepseek/deepseek-v3.2-speciale",
      "canonical_slug": "deepseek/deepseek-v3.2-speciale-20251201",
      "hugging_face_id": "deepseek-ai/DeepSeek-V3.2-Speciale",
      "name": "DeepSeek: DeepSeek V3.2 Speciale",
      "created": 1764594837,
      "description": "DeepSeek-V3.2-Speciale is a high-compute variant of DeepSeek-V3.2 optimized for maximum reasoning and agentic performance. It builds on DeepSeek Sparse Attention (DSA) for efficient long-context processing, then scales post-training reinforcement learning to push capability beyond the base model. Reported evaluations place Speciale ahead of GPT-5 on difficult reasoning workloads, with proficiency comparable to Gemini-3.0-Pro, while retaining strong coding and tool-use reliability. Like V3.2, it benefits from a large-scale agentic task synthesis pipeline that improves compliance and generalization in interactive environments.",
      "context_length": 163840,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "DeepSeek",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000027",
        "completion": "0.00000041",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 163840,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 1,
        "top_p": 0.95,
        "frequency_penalty": null
      }
    },
    {
      "id": "deepseek/deepseek-v3.2",
      "canonical_slug": "deepseek/deepseek-v3.2-20251201",
      "hugging_face_id": "deepseek-ai/DeepSeek-V3.2",
      "name": "DeepSeek: DeepSeek V3.2",
      "created": 1764594642,
      "description": "DeepSeek-V3.2 is a large language model designed to harmonize high computational efficiency with strong reasoning and agentic tool-use performance. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism that reduces training and inference cost while preserving quality in long-context scenarios. A scalable reinforcement learning post-training framework further improves reasoning, with reported performance in the GPT-5 class, and the model has demonstrated gold-medal results on the 2025 IMO and IOI. V3.2 also uses a large-scale agentic task synthesis pipeline to better integrate reasoning into tool-use settings, boosting compliance and generalization in interactive environments.\n\nUsers can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
      "context_length": 163840,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "DeepSeek",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000025",
        "completion": "0.00000038",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 163840,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 1,
        "top_p": 0.95,
        "frequency_penalty": null
      }
    },
    {
      "id": "x-ai/grok-4.1-fast",
      "canonical_slug": "x-ai/grok-4.1-fast",
      "hugging_face_id": "",
      "name": "xAI: Grok 4.1 Fast",
      "created": 1763587502,
      "description": "Grok 4.1 Fast is xAI's best agentic tool calling model that shines in real-world use cases like customer support and deep research. 2M context window.\n\nReasoning can be enabled/disabled using the `reasoning` `enabled` parameter in the API. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)",
      "context_length": 2000000,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": ["text", "image"],
        "output_modalities": ["text"],
        "tokenizer": "Grok",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000002",
        "completion": "0.0000005",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0",
        "input_cache_read": "0.00000005"
      },
      "top_provider": {
        "context_length": 2000000,
        "max_completion_tokens": 30000,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "logprobs",
        "max_tokens",
        "reasoning",
        "response_format",
        "seed",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 0.7,
        "top_p": 0.95,
        "frequency_penalty": null
      }
    },
    {
      "id": "nvidia/nemotron-nano-12b-v2-vl",
      "canonical_slug": "nvidia/nemotron-nano-12b-v2-vl",
      "hugging_face_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16",
      "name": "NVIDIA: Nemotron Nano 12B 2 VL",
      "created": 1761675565,
      "description": "NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning model designed for video understanding and document intelligence. It introduces a hybrid Transformer-Mamba architecture, combining transformer-level accuracy with Mamba’s memory-efficient sequence modeling for significantly higher throughput and lower latency.\n\nThe model supports inputs of text and multi-image documents, producing natural-language outputs. It is trained on high-quality NVIDIA-curated synthetic datasets optimized for optical-character recognition, chart reasoning, and multimodal comprehension.\n\nNemotron Nano 2 VL achieves leading results on OCRBench v2 and scores ≈ 74 average across MMMU, MathVista, AI2D, OCRBench, OCR-Reasoning, ChartQA, DocVQA, and Video-MME—surpassing prior open VL baselines. With Efficient Video Sampling (EVS), it handles long-form videos while reducing inference cost.\n\nOpen-weights, training data, and fine-tuning recipes are released under a permissive NVIDIA open license, with deployment supported across NeMo, NIM, and major inference runtimes.",
      "context_length": 131072,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": ["image", "text", "video"],
        "output_modalities": ["text"],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000002",
        "completion": "0.0000006",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      }
    },
    {
      "id": "google/gemini-2.5-flash",
      "canonical_slug": "google/gemini-2.5-flash",
      "hugging_face_id": "",
      "name": "Google: Gemini 2.5 Flash",
      "created": 1750172488,
      "description": "Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
      "context_length": 1048576,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": ["file", "image", "text", "audio", "video"],
        "output_modalities": ["text"],
        "tokenizer": "Gemini",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000003",
        "completion": "0.0000025",
        "request": "0",
        "image": "0.001238",
        "audio": "0.000001",
        "web_search": "0",
        "internal_reasoning": "0",
        "input_cache_read": "0.00000003",
        "input_cache_write": "0.0000003833"
      },
      "top_provider": {
        "context_length": 1048576,
        "max_completion_tokens": 65535,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      }
    },
    {
      "id": "openai/gpt-5.1",
      "canonical_slug": "openai/gpt-5.1-20251113",
      "hugging_face_id": "",
      "name": "OpenAI: GPT-5.1",
      "created": 1763060305,
      "description": "GPT-5.1 is the latest frontier-grade model in the GPT-5 series, offering stronger general-purpose reasoning, improved instruction adherence, and a more natural conversational style compared to GPT-5. It uses adaptive reasoning to allocate computation dynamically, responding quickly to simple queries while spending more depth on complex tasks. The model produces clearer, more grounded explanations with reduced jargon, making it easier to follow even on technical or multi-step problems.\n\nBuilt for broad task coverage, GPT-5.1 delivers consistent gains across math, coding, and structured analysis workloads, with more coherent long-form answers and improved tool-use reliability. It also features refined conversational alignment, enabling warmer, more intuitive responses without compromising precision. GPT-5.1 serves as the primary full-capability successor to GPT-5",
      "context_length": 400000,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": ["image", "text", "file"],
        "output_modalities": ["text"],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000125",
        "completion": "0.00001",
        "request": "0",
        "image": "0",
        "web_search": "0.01",
        "internal_reasoning": "0",
        "input_cache_read": "0.000000125"
      },
      "top_provider": {
        "context_length": 400000,
        "max_completion_tokens": 128000,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "response_format",
        "seed",
        "structured_outputs",
        "tool_choice",
        "tools"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      }
    },
    {
      "id": "openai/gpt-5-mini",
      "canonical_slug": "openai/gpt-5-mini-2025-08-07",
      "hugging_face_id": "",
      "name": "OpenAI: GPT-5 Mini",
      "created": 1754587407,
      "description": "GPT-5 Mini is a compact version of GPT-5, designed to handle lighter-weight reasoning tasks. It provides the same instruction-following and safety-tuning benefits as GPT-5, but with reduced latency and cost. GPT-5 Mini is the successor to OpenAI's o4-mini model.",
      "context_length": 400000,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": ["text", "image", "file"],
        "output_modalities": ["text"],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000025",
        "completion": "0.000002",
        "request": "0",
        "image": "0",
        "web_search": "0.01",
        "internal_reasoning": "0",
        "input_cache_read": "0.000000025"
      },
      "top_provider": {
        "context_length": 400000,
        "max_completion_tokens": 128000,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "response_format",
        "seed",
        "structured_outputs",
        "tool_choice",
        "tools"
      ],
      "default_parameters": {}
    },
    {
      "id": "deepseek/deepseek-v3.2-exp",
      "canonical_slug": "deepseek/deepseek-v3.2-exp",
      "hugging_face_id": "deepseek-ai/DeepSeek-V3.2-Exp",
      "name": "DeepSeek: DeepSeek V3.2 Exp",
      "created": 1759150481,
      "description": "DeepSeek-V3.2-Exp is an experimental large language model released by DeepSeek as an intermediate step between V3.1 and future architectures. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism designed to improve training and inference efficiency in long-context scenarios while maintaining output quality. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model was trained under conditions aligned with V3.1-Terminus to enable direct comparison. Benchmarking shows performance roughly on par with V3.1 across reasoning, coding, and agentic tool-use tasks, with minor tradeoffs and gains depending on the domain. This release focuses on validating architectural optimizations for extended context lengths rather than advancing raw task accuracy, making it primarily a research-oriented model for exploring efficient transformer designs.",
      "context_length": 163840,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "DeepSeek",
        "instruct_type": "deepseek-v3.1"
      },
      "pricing": {
        "prompt": "0.00000021",
        "completion": "0.00000032",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 163840,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 0.6,
        "top_p": 0.95,
        "frequency_penalty": null
      }
    },
    {
      "id": "deepseek/deepseek-r1-0528",
      "canonical_slug": "deepseek/deepseek-r1-0528",
      "hugging_face_id": "deepseek-ai/DeepSeek-R1-0528",
      "name": "DeepSeek: R1 0528",
      "created": 1748455170,
      "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.",
      "context_length": 163840,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "DeepSeek",
        "instruct_type": "deepseek-r1"
      },
      "pricing": {
        "prompt": "0.0000004",
        "completion": "0.00000175",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 163840,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {}
    },
    {
      "id": "z-ai/glm-4.6",
      "canonical_slug": "z-ai/glm-4.6",
      "hugging_face_id": "",
      "name": "Z.AI: GLM 4.6",
      "created": 1759235576,
      "description": "Compared with GLM-4.5, this generation brings several key improvements:\n\nLonger context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\nSuperior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.\nAdvanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\nMore capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\nRefined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.",
      "context_length": 202752,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000035",
        "completion": "0.0000015",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 202752,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_a",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 0.6,
        "top_p": null,
        "frequency_penalty": null
      }
    },
    {
      "id": "google/gemini-2.5-flash-image",
      "canonical_slug": "google/gemini-2.5-flash-image",
      "hugging_face_id": "",
      "name": "Google: Gemini 2.5 Flash Image (Nano Banana)",
      "created": 1759870431,
      "description": "Gemini 2.5 Flash Image, a.k.a. \"Nano Banana,\" is now generally available. It is a state of the art image generation model with contextual understanding. It is capable of image generation, edits, and multi-turn conversations. Aspect ratios can be controlled with the [image_config API Parameter](https://openrouter.ai/docs/features/multimodal/image-generation#image-aspect-ratio-configuration)",
      "context_length": 32768,
      "architecture": {
        "modality": "text+image->text+image",
        "input_modalities": ["image", "text"],
        "output_modalities": ["image", "text"],
        "tokenizer": "Gemini",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000003",
        "completion": "0.0000025",
        "request": "0",
        "image": "0.001238",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 32768,
        "max_completion_tokens": 32768,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "response_format",
        "seed",
        "structured_outputs",
        "temperature",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      }
    },
    {
      "id": "qwen/qwen3-235b-a22b",
      "canonical_slug": "qwen/qwen3-235b-a22b-04-28",
      "hugging_face_id": "Qwen/Qwen3-235B-A22B",
      "name": "Qwen: Qwen3 235B A22B",
      "created": 1745875757,
      "description": "Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B parameters per forward pass. It supports seamless switching between a \"thinking\" mode for complex reasoning, math, and code tasks, and a \"non-thinking\" mode for general conversational efficiency. The model demonstrates strong reasoning ability, multilingual support (100+ languages and dialects), advanced instruction-following, and agent tool-calling capabilities. It natively handles a 32K token context window and extends up to 131K tokens using YaRN-based scaling.",
      "context_length": 40960,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Qwen3",
        "instruct_type": "qwen3"
      },
      "pricing": {
        "prompt": "0.00000018",
        "completion": "0.00000054",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 40960,
        "max_completion_tokens": 40960,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {}
    },
    {
      "id": "deepseek/deepseek-r1-distill-qwen-32b",
      "canonical_slug": "deepseek/deepseek-r1-distill-qwen-32b",
      "hugging_face_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
      "name": "DeepSeek: R1 Distill Qwen 32B",
      "created": 1738194830,
      "description": "DeepSeek R1 Distill Qwen 32B is a distilled large language model based on [Qwen 2.5 32B](https://huggingface.co/Qwen/Qwen2.5-32B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\n\\nOther benchmark results include:\\n\\n- AIME 2024 pass@1: 72.6\\n- MATH-500 pass@1: 94.3\\n- CodeForces Rating: 1691\\n\\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": ["text"],
        "output_modalities": ["text"],
        "tokenizer": "Qwen",
        "instruct_type": "deepseek-r1"
      },
      "pricing": {
        "prompt": "0.00000027",
        "completion": "0.00000027",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_k",
        "top_p"
      ],
      "default_parameters": {}
    },
    {
      "id": "google/gemini-3-pro-preview",
      "canonical_slug": "google/gemini-3-pro-preview-20251117",
      "hugging_face_id": "",
      "name": "Google: Gemini 3 Pro Preview",
      "created": 1763474668,
      "description": "Gemini 3 Pro is Google’s flagship frontier model for high-precision multimodal reasoning, combining strong performance across text, image, video, audio, and code with a 1M-token context window. Reasoning Details must be preserved when using multi-turn tool calling, see our docs here: https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks. It delivers state-of-the-art benchmark results in general reasoning, STEM problem solving, factual QA, and multimodal understanding, including leading scores on LMArena, GPQA Diamond, MathArena Apex, MMMU-Pro, and Video-MMMU. Interactions emphasize depth and interpretability: the model is designed to infer intent with minimal prompting and produce direct, insight-focused responses.\n\nBuilt for advanced development and agentic workflows, Gemini 3 Pro provides robust tool-calling, long-horizon planning stability, and strong zero-shot generation for complex UI, visualization, and coding tasks. It excels at agentic coding (SWE-Bench Verified, Terminal-Bench 2.0), multimodal analysis, and structured long-form tasks such as research synthesis, planning, and interactive learning experiences. Suitable applications include autonomous agents, coding assistants, multimodal analytics, scientific reasoning, and high-context information processing.",
      "context_length": 1048576,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": ["text", "image", "file", "audio", "video"],
        "output_modalities": ["text"],
        "tokenizer": "Gemini",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.000002",
        "completion": "0.000012",
        "request": "0",
        "image": "0.008256",
        "web_search": "0",
        "internal_reasoning": "0",
        "input_cache_read": "0.0000002",
        "input_cache_write": "0.000002375"
      },
      "top_provider": {
        "context_length": 1048576,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      }
    },
    {
      "id": "google/gemini-3-pro-image-preview",
      "canonical_slug": "google/gemini-3-pro-image-preview-20251120",
      "hugging_face_id": "",
      "name": "Google: Nano Banana Pro (Gemini 3 Pro Image Preview)",
      "created": 1763653797,
      "description": "Nano Banana Pro is Google’s most advanced image-generation and editing model, built on Gemini 3 Pro. It extends the original Nano Banana with significantly improved multimodal reasoning, real-world grounding, and high-fidelity visual synthesis. The model generates context-rich graphics, from infographics and diagrams to cinematic composites, and can incorporate real-time information via Search grounding.\n\nIt offers industry-leading text rendering in images (including long passages and multilingual layouts), consistent multi-image blending, and accurate identity preservation across up to five subjects. Nano Banana Pro adds fine-grained creative controls such as localized edits, lighting and focus adjustments, camera transformations, and support for 2K/4K outputs and flexible aspect ratios. It is designed for professional-grade design, product visualization, storyboarding, and complex multi-element compositions while remaining efficient for general image creation workflows.",
      "context_length": 65536,
      "architecture": {
        "modality": "text+image->text+image",
        "input_modalities": ["image", "text"],
        "output_modalities": ["image", "text"],
        "tokenizer": "Gemini",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.000002",
        "completion": "0.000012",
        "request": "0",
        "image": "0.067",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 65536,
        "max_completion_tokens": 32768,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      }
    },
    {
      "id": "google/gemini-2.5-flash-lite-preview-09-2025",
      "canonical_slug": "google/gemini-2.5-flash-lite-preview-09-2025",
      "hugging_face_id": "",
      "name": "Google: Gemini 2.5 Flash Lite Preview 09-2025",
      "created": 1758819686,
      "description": "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ",
      "context_length": 1048576,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": ["text", "image", "file", "audio", "video"],
        "output_modalities": ["text"],
        "tokenizer": "Gemini",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000001",
        "completion": "0.0000004",
        "request": "0",
        "image": "0",
        "web_search": "0",
        "internal_reasoning": "0"
      },
      "top_provider": {
        "context_length": 1048576,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      }
    }
  ]
}
